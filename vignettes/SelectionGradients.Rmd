---
title: "Comparing selection gradients, direct selection on phenotypic traits"
author: "Sandy Kawano"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{SelectionGradients}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Install tremors from GitHub
```{r}
require(devtools)
install_github("MorphoFun/tremors")
library(tremors)
```

To see all of the vignettes that are available in `tremors`, type `browseVignettes("tremors")`. 

## Loading the Male Bumpus data as an example
```{r}
data(BumpusMales)
```

## Check the structure of the data
```{r}
str(BumpusMales)
```

## Check the documentation to understand what each of the 13 columns represent 
```{r}
?BumpusMales
```

## Define the parameters
#### Fitness will be represented by the relative fitness (w), which is the absolute fitness (W) of an individual divided by the mean absolute fitness for the group.
```{r}
w <- BumpusMales[,1]
```

#### The phenotypic traits are stored under z:
```{r}
z <- BumpusMales[,3:11]
```

#### Since Weight is a volume, and the other traits are linear measurements, we need to adjust for the dimensionality differences by taking the cube root of Weight.g. (Alternatively, one could cube all of the other phenotypic traits except Weight.g)
```{r}
z$Weight.g <- z$Weight.g^(1/3)
```

## Calculate the selection gradients, using the Lande-Arnold Method
#### This can be implemented by using tremors::glam
The benefit of using tremors::glam over stats::lm or stats::glm is that glam has built-in options to make the regression coefficients more directly comparable to selection gradients. There are three main benefits to using glam:
1. glm will check whether the data are centered to a mean of zero and standardized to unit variance prior, and will proceed with these standardizations as a default if raw data are entered into the function.   
1. Both linear and nonlinear selection are evaluated, and are output in separate lists. "GL" = gradients for linear selection, and "GNL" = gradients for nonlinear selection. The linear terms in the nonlinear selection analysis should be ignored, but represent biased estimates of linear selection and should be ignored. Linear selection gradients should only be evaluated in the GL, which implements only a linear-terms analysis.    
1. Quadratic regression coefficients and their associated standard errors from standard statistical programs need to be doubled in order to generate appropriate quadratic selection gradients and standard errors. Such an oversight has resulted in substantial underestimation of the strength of quadratic selection (Stincombe et al. 2008). However, glam automatically accounts for this discrepancy, so that the outputted coefficients represent the selection gradients, and do not require further modification.    
1. There is also an option to apply the Janzen and Stern (1998) correction factor to convert logistic regression coefficients to comparable selection gradients and, again, quadratic regression coefficients do not need to be doubled. glam will also automatically analyze both quadratic and correlational selection, so users can easily output a comprehensive dataset to publish results on both linear and nonlinear selection.       

To demonstrate the functionality of glam, we will be using the male sparrow data from the classic Bumpus (1968) study to reproduce the results from the Janzen and Stern (1998) paper that provided a comparison of the Lande-Arnold (1983) multiple linear regression method and the Janzen-Stern (1998) multiple logistic regression with a correction factor. The results calculated herein will replicate the results from Table 1 in Janzen and Stern (1998). The phenotypic traits (z) will be log-transformed to linearize the data, and was done in both Lande and Arnold (1983) and Janzen and Stern (1998).

#### Here are the results based on the Lande-Arnold method:  
```{r}
bmlm <- glam(w, log(z), fitType = "gaussian")

# Gradients for linear selection will be stored under object$GL
summary.glam(bmlm$GL)

# Note that the Estimate, Std. Error, and Pr(>|t|) columns of the summary output match the standardized beta (selection gradient), SE, and P columns of Table 1 in Janzen and Stern (1998) for the 'Linear regression analysis'.
```

#### Here are the results based on the Janzen-Stern method:
```{r}
# This is going to be a logistic regression, so the "W" (absolute fitness) column will be used for the fitness metric, the fitType will then be "binomial", and we're going to set JS = TRUE because we are performing the Janzen and Stern analysis.
bmjs <- glam(BumpusMales$W, log(z), fitType = "binomial", JS = TRUE)

```

You should have noticed that the following warning messages:
1: glm.fit: algorithm did not converge 
2: glm.fit: fitted probabilities numerically 0 or 1 occurred 
3: glm.fit: algorithm did not converge 
4: glm.fit: fitted probabilities numerically 0 or 1 occurred 
We will return to these warning messages later in the vignette. 

```{r}
# Gradients for linear selection will be stored under object$GL
summary.glam(bmjs$GL)

# Note that the Estimate, Std. Error, and Pr(>|t|) columns of the summary output match the beta avggrad (selection gradient), SE, and P columns of Table 1 in Janzen and Stern (1998) for the 'Logistic regression analysis'.

# The custom summary function in tremors must be used for analyses using the Janzen and Stern method because the base::summary function will not accurately calculate the t-value and, thus, p-values correctly given the transformed coefficients. This is because the Janzen and Stern method only affects the regression coefficients and their associated standard errors, but not the statistical testing. Yet, the summary function will calculate the p-value based on the t-value, which is calculated as a ratio of the regression coefficient to the standard error. 
```

Despite the fact that warning messages were issued for this analysis, glam still output the correct results for the Janzen and Stern approach for calculating linear selection gradients. The warning messages only apply to the nonlinear selection analysis, and occurs because the model is essentially overparamterized. What this means is that there are too few observations relative to the number of variables, and arises primarily because of the correlational/cross-product terms that increase precipitiously as the number of phenotypic traits increase. This is a common challenge in statistical analyses in all fields, and is often dubbed the 'n vs. p' problem.    

There is current disagreement over the best approach to account for the multi-dimensionality problem. Some have suggested using Principal Components Analysis (PCA) to reduce the dimensionality of the data (e.g., Lande and Arnodl 1983), but that can make it difficult to deduce a straight-forward biological interpretation of the data. Others have used the PCA to select a subset of traits that describe the most amount of variation in the phenotypic data by only analyzing the top 6 traits that load highest on PC 1 or PC 2 (Kawano et al. 2013). However, there do not appear to be any studies to suggest that natural selection operates in the same manner as a PCA, so this method may lose biological relevance of the data.  
