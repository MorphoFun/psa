---
title: "Comparing selection gradients: direct selection on phenotypic traits"
author: "Sandy Kawano"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{SelectionGradients}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
  %\usepackage[utf8]{inputenc}
---

## Install psa from GitHub
```{r}
require(devtools)
install_github("MorphoFun/psa", build_vignettes = TRUE)
library(psa)
```

To see all of the vignettes that are available in `psa`, type `browseVignettes("psa")`. 

## Loading the Male Bumpus data as an example
```{r}
data(BumpusMales)
```

## Check the structure of the data
```{r}
str(BumpusMales)
```

## Check the documentation to understand what each of the 13 columns represent 
```{r}
?BumpusMales
```

## Define the parameters
#### Fitness will be represented by the relative fitness (w), which is the absolute fitness (W) of an individual divided by the mean absolute fitness for the group.
```{r}
w <- BumpusMales[,1]
```

#### The phenotypic traits are stored under z:
```{r}
z <- BumpusMales[,3:11]
```

#### Since Weight is a volume, and the other traits are linear measurements, we need to adjust for the dimensionality differences by taking the cube root of Weight_g. (Alternatively, one could cube all of the other phenotypic traits except Weight_g)
```{r}
z$Weight_g <- z$Weight_g^(1/3)
```

## Calculate the selection gradients, using the Lande-Arnold Method
#### This can be implemented by using psa::glam
The benefit of using psa::glam over stats::lm or stats::glm is that glam has built-in options to make the regression coefficients more directly comparable to selection gradients. There are four main benefits to using glam:
1. glam will check whether the data are centered to a mean of zero and standardized to unit variance prior, and will proceed with these standardizations as a default if raw data are entered into the function.   
1. Both linear and nonlinear selection are evaluated, and are output in separate lists. "GL" = gradients for linear selection, and "GNL" = gradients for nonlinear selection. The linear terms in the nonlinear selection analysis should be ignored, but represent biased estimates of linear selection and should be ignored. Linear selection gradients should only be evaluated in the GL, which only conducts a first order model.    
1. Quadratic regression coefficients and their associated standard errors from standard statistical programs need to be doubled in order to generate appropriate quadratic selection gradients and standard errors. Such an oversight has resulted in substantial underestimation of the strength of quadratic selection (Stincombe et al. 2008). However, glam automatically accounts for this discrepancy, so that the outputted coefficients represent the selection gradients, and do not require further modification.    
1. There is also an option to apply the Janzen and Stern (1998) correction factor to convert logistic regression coefficients to comparable selection gradients and, again, quadratic regression coefficients do not need to be doubled. glam will also automatically analyze both quadratic and correlational selection using the Janzen and Stern modification, so users can easily output a comprehensive dataset to publish results on both linear and nonlinear selection.       

To demonstrate the functionality of glam, we will be using the male sparrow data from the classic Bumpus (1968) study to reproduce the results from the Janzen and Stern (1998) paper that provided a comparison of the Lande-Arnold (1983) multiple linear regression method and the Janzen-Stern (1998) multiple logistic regression with a correction factor. The results calculated herein will replicate the results from Table 1 in Janzen and Stern (1998). The phenotypic traits (z) will be log-transformed to linearize the data, and evaluated with both the Lande and Arnold (1983) and Janzen and Stern (1998) methods. 

#### Here are the results based on the Lande-Arnold method:  
```{r}
bmlm <- glam(w, log(z), fitType = "gaussian")

# Gradients for linear selection will be stored under object$GL
summary.glam(bmlm$GL)
confint(bmlm$GL)

# Note that the Estimate, Std. Error, and Pr(>|t|) columns of the summary output match the standardized beta (selection gradient), SE, and P columns of Table 1 in Janzen and Stern (1998) for the 'Linear regression analysis'.

# Gradients for nonlinear selection will be stored under object$GNL
summary.glam(bmlm$GNL)
confint(bmlm$GNL)

# Remember to ignore the linear terms from this second-order model; they do NOT represent the directional selection gradients. 
```

#### Here are the results based on the Janzen-Stern method:
```{r}
# This is going to be a logistic regression, so the "W" (absolute fitness) column will be used for the fitness metric, the fitType will then be "binomial", and we're going to set JS = TRUE because we are performing the Janzen and Stern analysis.
bmjs <- glam(BumpusMales$W, log(z), fitType = "binomial", JS = TRUE)

```

You should have noticed that the following warning messages:
1: glm.fit: algorithm did not converge 
2: glm.fit: fitted probabilities numerically 0 or 1 occurred 
3: glm.fit: algorithm did not converge 
4: glm.fit: fitted probabilities numerically 0 or 1 occurred 
We will return to these warning messages later in the vignette. 

```{r}
# Gradients for linear selection will be stored under object$GL
summary.glam(bmjs$GL, JS = TRUE)

# Note that the Estimate, Std. Error, and Pr(>|t|) columns of the summary output match the beta avggrad (selection gradient), SE, and P columns of Table 1 in Janzen and Stern (1998) for the 'Logistic regression analysis'.

# The custom summary function (psa::summary.glam) must be used for analyses using the Janzen and Stern method because the base::summary function will not accurately calculate the t-value and, thus, p-values correctly for the logistic regression coefficients using the Janzen and Stern method. This is because the Janzen and Stern method only affects the regression coefficients, but not the statistical testing. Yet, the summary function will calculate the p-value based on the t-value, which is calculated as a ratio of the regression coefficient to the standard error. 
```

Despite the fact that warning messages were issued for this analysis, glam still output the results for the Janzen and Stern approach for calculating linear selection gradients. The warning messages only apply to the nonlinear selection analysis, and occurs because the model is essentially overfit. Other big red flags are that the z values are all zero and the p-values are all one. Consequently, the estimates from this second order model suggest that the estimates from these nonlinear terms are unreliable. 

Overfitting can arise when there are too few observations relative to the number of variables, and arises primarily because of the correlational/cross-product terms that increase precipitiously as the number of phenotypic traits increase. This is a common challenge in statistical analyses in all fields, and is often dubbed the 'n vs. p' problem.    

There is current disagreement over the best approach to account for the multi-dimensionality problem. Some have suggested using Principal Components Analysis (PCA) to reduce the dimensionality of the data (e.g., Lande and Arnodl 1983), but that can make it difficult to deduce a straight-forward biological interpretation of the data. Others have used the PCA to select a subset of traits that describe the most amount of variation in the phenotypic data by only analyzing the top 6 traits that load highest on PC 1 or PC 2 (Kawano et al. 2013). However, there do not appear to be any studies to suggest that natural selection operates in the same manner as a PCA, so this method may lose biological relevance of the data. Others only analyze a subset of the traits that were measured, so that the model includes only a handful of traits that were considered to be important a prior. However, this could remove potentially important indirect interactions that could be under correlational selection... 

#### Evaluating the sample size needed to run the second order logistic regression model 

Peduzzi et al. (1996) recommended that the minimum sample size for a logistic regression model should be:
  N = (10 * k)/p
where N = the number of observations, k = number of covariates, and p = the lowest propertion of either of the dichotomous outcomes.

For the male Bumpus data, this would then be: 
```{r}
  k = length(BumpusMales[,3:11]) = 9 
  p = min(round(prop.table(table(BumpusMales$w)), digits = 2))
  N = (10 * k)/p
  N
```
Based on this general recommendation, there would need to be at least 220 observations in order to run a logistic regression model with such features. 

However, a quick example that increases the sample size of the BumpusMales data by 6x shows that there are still problems, so this error cannot be solved by simply increasing the sample size. 
```{r}
# Generating the data
bm6x <- data.frame(rbind(BumpusMales, BumpusMales, BumpusMales,BumpusMales, BumpusMales, BumpusMales))

#performing the Janzen and Stern analysis.
bmjs6x <- glam(bm6x$W, log(bm6x[,3:11]), fitType = "binomial", JS = TRUE)

summary.glam(bmjs6x$GL, JS = TRUE)
summary.glam(bmjs6x$GNL, JS = TRUE)

```
Peduzzi P, Concato J, Kemper E, Holford TR, Feinstein AR. 1996. A simulation study of the number of events per variable in logistic regression analysis. Journal of Clinical Epidemiology 49: 1373 - 1379.

